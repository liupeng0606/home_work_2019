import torch
from torch import nn
from torch.nn import functional as F
from torch.optim import Adam
import pandas as pd
import numpy as np
from torch.utils.data.dataloader import DataLoader
from scipy.stats import pearsonr

hidden_dim = 64
batch_size = 64
encode_len = 768
data_len = 100

# tarin_set
tarin_x_set = pd.read_csv("./data/train_encode.csv", header=None).values
tarin_y_set = pd.read_csv("./data/train_label.csv", header=0).values


print(len(tarin_x_set), len(tarin_y_set))

print(tarin_x_set.shape, tarin_y_set.shape)
train_x_y = list(zip(tarin_x_set, tarin_y_set))
train_x_y = list(map(lambda x: (torch.tensor(x[0],dtype=torch.float32),torch.tensor(x[1], dtype=torch.float32)), train_x_y))
print("train dataset len : ", len(train_x_y))

# tarin_set
test_x_set = pd.read_csv("./data/test_encode.csv", header=None).values
test_y_set = pd.read_csv("./data/test_label.csv", header=0).values
test_x_y = list(zip(test_x_set, test_y_set))
test_x_y = list(map(lambda x: (torch.tensor(x[0],dtype=torch.float32), torch.tensor(x[1], dtype=torch.float32)), test_x_y))
print("test dataset len : ", len(test_x_y))

# print(test_x_y[0])

# Train_DataLoader
train_data_loader = DataLoader(train_x_y, shuffle=True, batch_size = batch_size)
# Test_DataLoader
test_data_loader = DataLoader(test_x_y, batch_size = len(test_x_y))

# train_data_loader = iter(train_data_loader)

class generator(nn.Module):
    def __init__(self):
        super(generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(hidden_dim, 6000),
            nn.LeakyReLU(True),
            nn.Linear(6000, 3000),
            nn.LeakyReLU(True),
            nn.Linear(3000, 768),
        )
    def forward(self, x):
        return self.model(x)



class discriminitor(nn.Module):
    def __init__(self):
        super(discriminitor, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(encode_len, 3000),
            nn.LeakyReLU(True),
            nn.Linear(3000, 1000),
            nn.LeakyReLU(True),
            nn.Linear(1000, 100),
            nn.LeakyReLU(True),
        )
        self.sigma = nn.Sequential(
            self.model,
            nn.Linear(100, 1),
            nn.Sigmoid()
        )
        self.regression = nn.Sequential(
            self.model,
            nn.Linear(100, 2)
        )
    def forward(self, x):
        return self.sigma(x), self.regression(x)

G = generator()
D = discriminitor()

g_optim = Adam(G.parameters(), lr=2e-4, weight_decay=0.1)  #  L2
d1_optim = Adam(D.parameters(), lr=2e-5, weight_decay=0.1)
d2_optim = Adam(D.parameters(), lr=1e-4, weight_decay=0.1)


def Train(enpochs):
    for epoch in range(enpochs):
        #  train discriminiter
        # D.train()
        for _ in range(4):
            # on real data
            train_data_loader = DataLoader(train_x_y, shuffle=True, batch_size=batch_size, num_workers=2)
            data, label = iter(train_data_loader).__next__()
            d1_optim.zero_grad()
            discriminitor_loss = - (D(data)[0].sum())
            reg_loss = F.mse_loss(D(data)[1], label)
            real_data_loss = discriminitor_loss + reg_loss
            real_data_loss.backward()
            d1_optim.step()

            # on fake data
            d2_optim.zero_grad()
            noise = torch.randn(data.shape[0], hidden_dim)
            generate_fake_data = G(noise).detach()    # remember detach !!!
            fake_loss = D(generate_fake_data)[0].sum()
            fake_loss.backward()
            d2_optim.step()

        #  train generator
        noise = torch.randn(data.shape[0], hidden_dim)
        fake_data_pred = G(noise)
        g_loss = - (D(fake_data_pred)[0].sum())
        d1_optim.zero_grad()
        d2_optim.zero_grad()
        g_optim.zero_grad()
        g_loss.backward()
        g_optim.step()

        if epoch % 50 == 0:
            print("real_data_loss: ", discriminitor_loss.item())
            print("fake_data_loss: ", fake_loss.item())
            print("reg_loss: ",reg_loss.item())
            print("g_loss: ",g_loss.item())
            print("**********************************", epoch)
            # torch.save(D.state_dict(), "./model/" + str(epoch) + '.pkl')
            # D.eval()
            train_data_loader = DataLoader(train_x_y, shuffle=False, batch_size=len(train_x_y))
            for data, label in train_data_loader:
                reg_val = D(data)[1]
                # pre
                pre_v = reg_val[:, 0].tolist()
                pre_a = reg_val[:, 1].tolist()
                # true
                v_true = label[:, 0]
                a_true = label[:, 1]
                f = open("./pred_result/r.csv", "a+")
                print("epoch :" + str(epoch), file=f)
                print("train v pcc", pearsonr(pre_v, v_true), file=f)
                print("train a pcc", pearsonr(pre_a, a_true), file=f)
                f.close()



            for data, label in test_data_loader:
                reg_val = D(data)[1]
                # pre
                pre_v = reg_val[:, 0].tolist()
                pre_a = reg_val[:, 1].tolist()
                # true
                v_true = label[:, 0]
                a_true = label[:, 1]
                f = open("./pred_result/r.csv", "a+")
                print("v pcc", pearsonr(pre_v, v_true), file=f)
                print("a pcc", pearsonr(pre_a, a_true), file=f)
                print("\n", file=f)
                f.close()

Train(100000)
